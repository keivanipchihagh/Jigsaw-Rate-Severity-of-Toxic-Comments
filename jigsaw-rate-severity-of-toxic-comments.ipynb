{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Rate Severity of Toxic Comments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom string import printable, punctuation\nfrom itertools import groupby \n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:30.459353Z","iopub.execute_input":"2021-11-19T19:44:30.460248Z","iopub.status.idle":"2021-11-19T19:44:30.466158Z","shell.execute_reply.started":"2021-11-19T19:44:30.460195Z","shell.execute_reply":"2021-11-19T19:44:30.465553Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Since source of the training data is different, the column names are different as well.\n# Let's adjust them to avoid confusion\ntrain_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntrain_df.rename(columns = {\n    'id': 'comment_id',\n    'comment_text': 'text'\n}, inplace = True)\n\n# Original competition data\nval_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntest_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\n# Always shuffle to avoid unnecessary patterns in the data\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)\nval_df = val_df.sample(frac = 1).reset_index(drop = True)\ntest_df = test_df.sample(frac = 1).reset_index(drop = True)\n\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:30.492920Z","iopub.execute_input":"2021-11-19T19:44:30.493509Z","iopub.status.idle":"2021-11-19T19:44:31.918129Z","shell.execute_reply.started":"2021-11-19T19:44:30.493460Z","shell.execute_reply":"2021-11-19T19:44:31.917244Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### EDA: Love at First Sight ðŸ‘‰ðŸ‘ˆ\nI must feel comfortable with the data I will be working on in order to operate well. So let's do a quick EDA on our data from the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) competition. Starting by plotting the count of each toxicity type:","metadata":{}},{"cell_type":"code","source":"toxicity_types = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nzeros, ones = [], []\n\nfor toxic_type in toxicity_types:\n    value_counts = train_df[toxic_type].value_counts()\n    zeros.append(value_counts[0])\n    ones.append(value_counts[1])\n\nfig = plt.figure(figsize = (20, 5))\nplt.title('Toxicity Types count')\nplt.bar(toxicity_types, ones)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:31.920062Z","iopub.execute_input":"2021-11-19T19:44:31.920385Z","iopub.status.idle":"2021-11-19T19:44:32.168025Z","shell.execute_reply.started":"2021-11-19T19:44:31.920344Z","shell.execute_reply":"2021-11-19T19:44:32.167112Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Looking at the columns, we have the *comment_text* and its corresponding toxicity types. By plotting the toxicity types values we see they don't quite match up. We could combine all toxicity types into one new feature called **toxicity** by summing their values (So called *Feature Engineering*).\n\n**NOTE: What coefficients each type has is extremely important!**","metadata":{}},{"cell_type":"code","source":"toxicity_coefs = {\n    'toxic': 1,\n    'severe_toxic': 1,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 1\n}\n\n\ntrain_df['toxicity'] = sum([train_df[type] * coef for type, coef in toxicity_coefs.items()])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.169337Z","iopub.execute_input":"2021-11-19T19:44:32.169544Z","iopub.status.idle":"2021-11-19T19:44:32.181493Z","shell.execute_reply.started":"2021-11-19T19:44:32.169518Z","shell.execute_reply":"2021-11-19T19:44:32.180636Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"toxicity_values = train_df['toxicity'].value_counts()\n\nplt.figure(figsize = (20, 5))\nplt.title('Toxicity Level Distribution')\nplt.bar(toxicity_values.keys(), toxicity_values.values, color = 'g')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.184100Z","iopub.execute_input":"2021-11-19T19:44:32.185113Z","iopub.status.idle":"2021-11-19T19:44:32.419418Z","shell.execute_reply.started":"2021-11-19T19:44:32.185077Z","shell.execute_reply":"2021-11-19T19:44:32.418629Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Downsampling: Balancing our data\nInbalanced data can be very problematic. Yes, we can do a few tricks but the problem pretty much stands. Best way to reduce this effect is to balance our data. (The *accuracy* metric is not reccommended for inbalanced data)","metadata":{}},{"cell_type":"code","source":"cutoff = toxicity_values[0] // 5\n\n# Shuffle first to avoid any unnecessary patterns\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)\n\n# Perform cutoff\ntrain_df = train_df.iloc[:cutoff]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.420678Z","iopub.execute_input":"2021-11-19T19:44:32.420906Z","iopub.status.idle":"2021-11-19T19:44:32.476616Z","shell.execute_reply.started":"2021-11-19T19:44:32.420878Z","shell.execute_reply":"2021-11-19T19:44:32.475895Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Text Cleaning\nAs language models improve, text-cleaning is becoming less necessary, but that's not the case for all models. My strategy to overcome this is to start simple and test some cleaning methods to see if they help the model or not.","metadata":{"execution":{"iopub.status.busy":"2021-11-16T20:37:56.996117Z","iopub.execute_input":"2021-11-16T20:37:56.99644Z","iopub.status.idle":"2021-11-16T20:37:57.002838Z","shell.execute_reply.started":"2021-11-16T20:37:56.996406Z","shell.execute_reply":"2021-11-16T20:37:57.001704Z"}}},{"cell_type":"code","source":"HTML_TAG_PATTERN = r\"<.*?>\"\nEMAIL_PATTERN = r'(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])'\nURL_PATTERN = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\nABBR_VERB_DICT = {\n    \"aren't\" : \"are not\",\n    \"arent\" : \"are not\",\n    \"can't\" : \"cannot\",\n    \"cant\" : \"cannot\",\n    \"couldn't\" : \"could not\",\n    \"couldnt\" : \"could not\",\n    \"didn't\" : \"did not\",\n    \"didnt\" : \"did not\",\n    \"doesn't\" : \"does not\",\n    \"doesnt\" : \"does not\",\n    \"don't\" : \"do not\",\n    \"dont\" : \"do not\",\n    \"hadn't\" : \"had not\",\n    \"hasn't\" : \"has not\",\n    \"haven't\" : \"have not\",\n    \"havent\" : \"have not\",\n    \"he'd\" : \"he would\",\n    \"he'll\" : \"he will\",\n    \"he's\" : \"he is\",\n    \"i'd\" : \"I would\",\n    \"i'd\" : \"I had\",\n    \"i'll\" : \"I will\",\n    \"i'm\" : \"I am\",\n    \"isn't\" : \"is not\",\n    \"it's\" : \"it is\",\n    \"it'll\":\"it will\",\n    \"i've\" : \"I have\",\n    \"let's\" : \"let us\",\n    \"mightn't\" : \"might not\",\n    \"mightnt\" : \"might not\",\n    \"mustn't\" : \"must not\",\n    \"shan't\" : \"shall not\",\n    \"she'd\" : \"she would\",\n    \"she'll\" : \"she will\",\n    \"she's\" : \"she is\",\n    \"shouldn't\" : \"should not\",\n    \"shouldnt\" : \"should not\",\n    \"shld\": \"should\",\n    \"that's\" : \"that is\",\n    \"thats\" : \"that is\",\n    \"there's\" : \"there is\",\n    \"theres\" : \"there is\",\n    \"they'd\" : \"they would\",\n    \"they'll\" : \"they will\",\n    \"they're\" : \"they are\",\n    \"theyre\":  \"they are\",\n    \"they've\" : \"they have\",\n    \"we'd\" : \"we would\",\n    \"we're\" : \"we are\",\n    \"weren't\" : \"were not\",\n    \"we've\" : \"we have\",\n    \"what'll\" : \"what will\",\n    \"what're\" : \"what are\",\n    \"what's\" : \"what is\",\n    \"what've\" : \"what have\",\n    \"where's\" : \"where is\",\n    \"who'd\" : \"who would\",\n    \"who'll\" : \"who will\",\n    \"who're\" : \"who are\",\n    \"who's\" : \"who is\",\n    \"who've\" : \"who have\",\n    \"won't\" : \"will not\",\n    \"wouldn't\" : \"would not\",\n    \"you'd\" : \"you would\",\n    \"you'll\" : \"you will\",\n    \"you're\" : \"you are\",\n    \"you've\" : \"you have\",\n    \"'re\": \" are\",\n    \"wasn't\": \"was not\",\n    \"we'll\":\" will\",    \n}\n\ndef remove_html_tags(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = HTML_TAG_PATTERN, repl = replace_with, string = string)\n\ndef fix_verb_abbr(string: str) -> str:\n    return ' '.join([ABBR_VERB_DICT[word.lower()] if (word.lower() in ABBR_VERB_DICT.keys()) else word for word in string.split()])\n\ndef remove_special_characters(string: str) -> str:\n    return ''.join(filter(lambda x: x in printable, string))\n\ndef remove_urls(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = URL_PATTERN, repl = replace_with, string = string)\n\ndef remove_emails(string: str, replace_with: str = '') -> str:\n    return re.sub(EMAIL_PATTERN, replace_with, string)\n\ndef remove_punctuation(string: str, punctuations: str) -> str:    \n    return string.translate(str.maketrans('', '', punctuations))\n\ndef remove_repeated_punctuations(string: str) -> str:\n    def replacement(match):\n        match = match.group()\n        return match[0] + (\" \" if \" \" in match else \"\")\n    return re.sub(r'[!\\\"#$%&\\'()*+,\\-.\\/:;<=>?@\\[\\\\\\]^_`{|}~ ]{2,}', replacement, string)\n\n\ndef clean_text(text):\n    \n    text = str(text)\n    \n     # Remove double quotations\n    text = text.replace('\"\"', '\"')\n    \n    # Replace new lines (\\n) with '.' and later remove consecutive repeated punctuations\n    text = text.replace('\\n', '. ')    \n    \n    text = remove_html_tags(text)                           # Remove HTML tags\n    text = remove_emails(text)                              # Remove email addresses\n    text = remove_urls(text)                                # Remove URLs\n#     text = fix_verb_abbr(text)                              # Fix verb abbreviations    \n    text = remove_special_characters(text)                  # Remove special characters\n    text = remove_repeated_punctuations(text)               # Remove consecutive repeated punctuations    \n    \n    # Strip leading and trailin puctuations and white spaces\n#     text = text.strip(punctuation).strip()\n    \n#     text = remove_punctuation(text, punctuations = '*~')    # Remove spesific puntuations\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.478129Z","iopub.execute_input":"2021-11-19T19:44:32.478339Z","iopub.status.idle":"2021-11-19T19:44:32.501257Z","shell.execute_reply.started":"2021-11-19T19:44:32.478313Z","shell.execute_reply":"2021-11-19T19:44:32.500431Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# for df in [train_df, test_df]:\n#     df['text'] = df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.502311Z","iopub.execute_input":"2021-11-19T19:44:32.502960Z","iopub.status.idle":"2021-11-19T19:44:32.510455Z","shell.execute_reply.started":"2021-11-19T19:44:32.502925Z","shell.execute_reply":"2021-11-19T19:44:32.509705Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# print(train_df['text'].values[:50])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.511995Z","iopub.execute_input":"2021-11-19T19:44:32.512305Z","iopub.status.idle":"2021-11-19T19:44:32.521643Z","shell.execute_reply.started":"2021-11-19T19:44:32.512262Z","shell.execute_reply":"2021-11-19T19:44:32.520726Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Validation function\nWe need some sort of validation for our model to monitor its behavior. One way to accomplish this is to use the data from *validation_data.csv*. We can use our model to predict on both *less_toxic* and *more_toxic* columns and comparing the results.","metadata":{}},{"cell_type":"code","source":"# Defining the validation function\ndef validate_model(model, vectorizer, less_toxic, more_toxic):\n    \n    # Vectorize\n    less_toxic = vectorizer.transform(less_toxic)\n    more_toxic = vectorizer.transform(more_toxic)\n    \n    # Make predictions\n    prob_1 = model.predict_proba(less_toxic)\n    prob_2 = model.predict_proba(more_toxic)\n    \n    return (prob_1[:, 1] < prob_2[:, 1]).mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.522663Z","iopub.execute_input":"2021-11-19T19:44:32.523200Z","iopub.status.idle":"2021-11-19T19:44:32.531837Z","shell.execute_reply.started":"2021-11-19T19:44:32.523164Z","shell.execute_reply":"2021-11-19T19:44:32.531148Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Balancing the Data","metadata":{}},{"cell_type":"code","source":"train_df['smoothed_toxicity'] = train_df['toxicity'].apply(lambda x: 1 if x > 0 else 0)\n\ncutoff = train_df['smoothed_toxicity'].value_counts().min()\nlabel_0 = train_df.loc[train_df['smoothed_toxicity'] == 0].sample(cutoff)\nlabel_1 = train_df.loc[train_df['smoothed_toxicity'] == 1]\ntrain_df = pd.concat([label_0, label_1])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.533955Z","iopub.execute_input":"2021-11-19T19:44:32.534340Z","iopub.status.idle":"2021-11-19T19:44:32.580245Z","shell.execute_reply.started":"2021-11-19T19:44:32.534301Z","shell.execute_reply":"2021-11-19T19:44:32.579528Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Vectorizing Data","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words = 'english')\nX_train = vectorizer.fit_transform(train_df['text'])\n\ny_train = train_df['smoothed_toxicity']","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:32.581324Z","iopub.execute_input":"2021-11-19T19:44:32.581539Z","iopub.status.idle":"2021-11-19T19:44:33.032015Z","shell.execute_reply.started":"2021-11-19T19:44:32.581513Z","shell.execute_reply":"2021-11-19T19:44:33.031086Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Build the model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n\n# Validate\nvalidate_model(model, vectorizer, val_df['less_toxic'], val_df['more_toxic'])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:33.033200Z","iopub.execute_input":"2021-11-19T19:44:33.033533Z","iopub.status.idle":"2021-11-19T19:44:37.455205Z","shell.execute_reply.started":"2021-11-19T19:44:33.033496Z","shell.execute_reply":"2021-11-19T19:44:37.454625Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Creating Submission","metadata":{}},{"cell_type":"code","source":"X_test = vectorizer.transform(test_df['text'])\ny_pred = model.predict_proba(X_test)\n\nsubmission_df = pd.DataFrame(data = {\n    'comment_id': test_df['comment_id'],\n    'score': y_pred[:, 1]\n}).to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:44:37.456117Z","iopub.execute_input":"2021-11-19T19:44:37.456673Z","iopub.status.idle":"2021-11-19T19:44:38.047454Z","shell.execute_reply.started":"2021-11-19T19:44:37.456642Z","shell.execute_reply":"2021-11-19T19:44:38.046759Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}