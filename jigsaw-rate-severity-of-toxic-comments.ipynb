{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport gc\nimport scipy\nimport numpy as np\nimport pandas as pd\nfrom copy import deepcopy\nfrom string import printable\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import Ridge, ElasticNet\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:12.189530Z","iopub.execute_input":"2021-11-25T05:06:12.189981Z","iopub.status.idle":"2021-11-25T05:06:13.443510Z","shell.execute_reply.started":"2021-11-25T05:06:12.189863Z","shell.execute_reply":"2021-11-25T05:06:13.442709Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Some constants that we use in several parts of our notebook\nRANDOM_STATE = 201\nSTOPWORDS = set(STOPWORDS)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.445559Z","iopub.execute_input":"2021-11-25T05:06:13.445912Z","iopub.status.idle":"2021-11-25T05:06:13.452016Z","shell.execute_reply.started":"2021-11-25T05:06:13.445866Z","shell.execute_reply":"2021-11-25T05:06:13.450760Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Strategy\nThere are three datasets introduced in the competition page. I will be using all three to build an Emsemble model. Datasets used in the notebook:\n- [jigsaw-toxic-comment-classification-challenge](https://www.kaggle.com/julian3833/jigsaw-toxic-comment-classification-challenge)\n- [jigsaw-unintended-bias-in-toxicity-classification](https://www.kaggle.com/julian3833/jigsaw-unintended-bias-in-toxicity-classification)","metadata":{}},{"cell_type":"markdown","source":"## Weights\nHere I have defined a dictionary that will map toxicity types to their corresponding weights. These weights are one of the most important parameters in the entire notebook.","metadata":{}},{"cell_type":"code","source":"# Toxicity weights - These weights are later used to combine all toxicity types into one\ntoxicity_weights = {\n    'toxic': 1,\n    'severe_toxic': 2,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 2,\n    'sexual_explicit': 1\n}\n\ntoxicity_types = list(toxicity_weights.keys())","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.453569Z","iopub.execute_input":"2021-11-25T05:06:13.453895Z","iopub.status.idle":"2021-11-25T05:06:13.472487Z","shell.execute_reply.started":"2021-11-25T05:06:13.453843Z","shell.execute_reply":"2021-11-25T05:06:13.471730Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Down-Sampling\nA function to downsample a given dataframe by a *threshold*.","metadata":{}},{"cell_type":"code","source":"def downsample(df, threshold, col = 'toxicity', cutoff_weight = 1.5):\n    \n    # Create cutoff\n    cutoff = int((df[col] > threshold).sum() * cutoff_weight)\n\n    # Crate downsampled df\n    downsampled_df = df[df[col] <= threshold].sample(cutoff, random_state = RANDOM_STATE)\n\n    # Concatenate and return the two dataframes\n    return pd.concat([downsampled_df, df[df[col] > threshold]])","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.474745Z","iopub.execute_input":"2021-11-25T05:06:13.475517Z","iopub.status.idle":"2021-11-25T05:06:13.488176Z","shell.execute_reply.started":"2021-11-25T05:06:13.475465Z","shell.execute_reply":"2021-11-25T05:06:13.487358Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Text Cleaning Methods\nAs newer language models and techniques come into play, text-cleaning is becoming less and less necessary and more like an option to include in our proces. But let's not forget that text-cleaning can still be of great importance in many models and scenarios. I have defined a number of functions that will help clean parts of our texts and have later on used a few I believed to be the most helpful of all.","metadata":{}},{"cell_type":"code","source":"HTML_TAG_PATTERN = r\"<.*?>\"\nEMAIL_PATTERN = r'(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])'\nURL_PATTERN = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n\ndef remove_html_tags(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = HTML_TAG_PATTERN, repl = replace_with, string = string)\n\ndef remove_special_characters(string: str) -> str:\n    return ''.join(filter(lambda x: x in printable, string))\n\ndef remove_urls(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = URL_PATTERN, repl = replace_with, string = string)\n\ndef remove_emails(string: str, replace_with: str = '') -> str:\n    return re.sub(EMAIL_PATTERN, replace_with, string)\n\ndef remove_repeated_punctuations(string: str) -> str:\n    def replacement(match):\n        match = match.group()\n        return match[0] + (\" \" if \" \" in match else \"\")\n    return re.sub(r'[!\\\"#$%&\\'()*+,\\-.\\/:;<=>?@\\[\\\\\\]^_`{|}~ ]{2,}', replacement, string)\n\n# Removes times and IP addresses\ndef remove_IPs(text):\n    return re.sub(r'(([0-9]+\\.){2,}[0-9]+)', '', text)        # 71.228.77.211\n\ndef remove_times(text):\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2} [a-zA-Z]+,? \\d{4} \\(UTC\\)', '', text)    # 04:09, 11 Jul, 2003  \n    text = re.sub(r'\\d{1,2}:\\d{2},? [a-zA-Z]+ \\d{1,2},? \\d{4} \\(UTC\\)', '', text)    # 16:47, Jul 23, 2004\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{4} [a-zA-Z]+ \\d{1,2},? \\(UTC\\)', '', text)    # 22:07, 2004 Dec 30\n    text = re.sub(r'\\d{1,2} [a-zA-Z]+ \\d{4},? \\d{1,2}:\\d{2} \\(UTC\\)', '', text)      # 29 June 2005 22:08\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2} [a-zA-Z]+,?', '', text)                  # 21:31, 6 April\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2},?', '', text)                            # 17:52, 12\n    text = re.sub(r'\\d{1,2}:\\d{1,2}-\\d{1,2}-\\d{1,2}', '', text)                      # 01:05-09-09    \n    text = re.sub(r'\\d{1,2}:\\d{2}', '', text)                                        # 17:52, 12  \n    \n    text = re.sub(r'\\(UTC\\)', '', text)                                              # (UTC)\n    return text\n\n# Replace repeating characters more than 3 times to length of 3\ndef shorten_repeated_patterns(text):\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n    \n    # Add space around repeated characters\n    text = re.sub(r'[ ]{2,}',' ', text).strip()\n    text = re.sub(r'([*!?\\']+)',r' \\1 ', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.489627Z","iopub.execute_input":"2021-11-25T05:06:13.490165Z","iopub.status.idle":"2021-11-25T05:06:13.505356Z","shell.execute_reply.started":"2021-11-25T05:06:13.490132Z","shell.execute_reply":"2021-11-25T05:06:13.504493Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):    \n    text = remove_html_tags(text)\n    text = remove_emails(text)\n    text = remove_urls(text)\n    text = remove_times(text)\n    rext = remove_IPs\n    text = remove_special_characters(text)\n    text = shorten_repeated_patterns(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.506643Z","iopub.execute_input":"2021-11-25T05:06:13.507317Z","iopub.status.idle":"2021-11-25T05:06:13.522942Z","shell.execute_reply.started":"2021-11-25T05:06:13.507285Z","shell.execute_reply":"2021-11-25T05:06:13.522298Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Fit, Validate and Predict\nWe don't have the straight-forward validation data as we normally have, so we must come on with a method to validate the models. I will be using the *validation.csv* which has two columns: *less_toxic* and *more_toxic*. As the result we must calculate the *Accuracy*.\n\nI will predict on each of the two columns and then computer **RMSE** and **Accuracy** metrics. This is better be done using StratifiedKFold to ensure that each fold of dataset has the same proportion of observations with a given labels. ([Read More](https://stackoverflow.com/questions/65318931/stratifiedkfold-vs-kfold-in-scikit-learn))\n\n\n**NOTE #1**: *Accuracy* can be misleading and is not a recommended-metric here as our data is strongly unbalanced! ([why not?](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/)) That's why I also use the RMSE.","metadata":{}},{"cell_type":"code","source":"# Calculate RMSE and Accuracy metrics\ndef validate(pipe, X_val, y_val):\n    ''' Pipe must have been fitted before being passed to this function '''\n    \n    # RMSE\n    rmse = mean_squared_error(pipe.predict(X_val), y_val, squared = False) \n\n    # Accuracy\n    lt_pred = pipe.predict(val_df['less_toxic'])\n    mt_pred = pipe.predict(val_df['more_toxic'])\n    accuracy = (lt_pred < mt_pred).mean()\n    \n    return lt_pred, mt_pred, accuracy, rmse","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.524336Z","iopub.execute_input":"2021-11-25T05:06:13.524795Z","iopub.status.idle":"2021-11-25T05:06:13.535368Z","shell.execute_reply.started":"2021-11-25T05:06:13.524760Z","shell.execute_reply":"2021-11-25T05:06:13.534580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def fit_validate_predict(pipe, X, y, folds = 5):\n    \n    # Created folds\n    skf = KFold(\n        n_splits = folds,\n        shuffle = True, # Default is False\n        random_state = RANDOM_STATE\n    )\n    accuracies, rmses = np.zeros(folds), np.zeros(folds)\n    lt_preds, mt_preds = np.zeros((val_df.shape[0], folds)), np.zeros((val_df.shape[0], folds))\n    preds = np.zeros((test_df.shape[0], folds))    \n    \n    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n        \n        # Split the data into train and test sets\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        \n        # Train the pipeline\n        pipe.fit(X_train, y_train)\n        \n        # Validate the pipeline with test_df['text'] and y_val\n        lt_pred, mt_pred, accuracy, rmse = validate(pipe, X_val, y_val)\n        accuracies[fold], rmses[fold] = accuracy, rmse\n        lt_preds[:, fold], mt_preds[:, fold] = lt_pred, mt_pred\n        \n        # Make predictions\n        preds[:, fold] = pipe.predict(test_df['text'])\n        \n        print(f\"FOLD #{fold + 1}) Accuracy: {accuracy.round(4)}, RMSE: {rmse.round(4)}\")\n    print(f\"\\n- Avg Accuracy: {accuracies.round(4).mean()}\\n- Avg RMSE: {rmses.round(4).mean()}\")\n    \n    return lt_preds, mt_preds, preds","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.536545Z","iopub.execute_input":"2021-11-25T05:06:13.536786Z","iopub.status.idle":"2021-11-25T05:06:13.551061Z","shell.execute_reply.started":"2021-11-25T05:06:13.536756Z","shell.execute_reply":"2021-11-25T05:06:13.550094Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Visualizations\nSince I'll (probably) be using multiple datasets in this notebook and run pretty much the same analysis over them, I'll define a few methods to avoid code duplication.","metadata":{}},{"cell_type":"code","source":"# Plots number of values for each toxicity level in the given dataframe\ndef plot_toxic_types_dist(df):    \n    fig = plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Categories Count')\n    plt.bar([type for type in toxicity_types if type in jtc_df.columns], [df[type].value_counts()[1] for type in toxicity_types if type in df.columns], label = 'Number of occurrences')\n    plt.legend()\n    plt.show()\n\n# Plots the didtribution of values in toxicity columns of the given dataframe\ndef plot_toxicity_dist(df):\n    toxicity_values = df['toxicity'].value_counts()\n    \n    plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Level Distribution')\n    plt.bar(toxicity_values.keys(), toxicity_values.values, color = 'g')\n    plt.show()\n\n# Plots the wordcloud for each toxicity level of the given data frame (Stopwords are removed)\ndef plot_wordcloud(df):\n    wordcloud = WordCloud(stopwords = STOPWORDS)\n    fig, ax = plt.subplots(3, 2, figsize = (20, 10))\n\n    i = 0\n    for row in ax:\n        for col in row:        \n            wordcloud.generate(' '.join(df.loc[df[toxicity_types[i]] != 0, 'text'].tolist()))\n            col.set_title(toxicity_types[i])        \n            col.imshow(wordcloud)        \n            col.axis(\"off\")\n            i += 1\n    plt.tight_layout(pad = 0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.552754Z","iopub.execute_input":"2021-11-25T05:06:13.553056Z","iopub.status.idle":"2021-11-25T05:06:13.570375Z","shell.execute_reply.started":"2021-11-25T05:06:13.553013Z","shell.execute_reply":"2021-11-25T05:06:13.569487Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Jigsaw Rate Severity of Toxic Comments\nThis is our original dataset for the competition. The columns are:\n- *comment_to_score.csv*: The dataset that is used for the final predictions.\n- *validation_data.csv*: The dataset that is used to validate the models.\n- *sample_submission.csv*: A sample submission file.\n\n**STEP #1**: Our validation contain duplicate (*less_toxic*, *more_toxic*) pairs. This won't be problematic for our metrics (metric improvement matters not its specific value), but I will remove the duplicates anyway.\n\n**STEP #2**: I will remove the *email addresses*, *html tags*, *URLs*, *times* and *IP addresses*.","metadata":{}},{"cell_type":"code","source":"val_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntest_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\nprint(f'test_df\\n- Shape: {test_df.shape}\\n- Columns: {list(test_df.columns)}')\nprint(f'- Duplicates: {test_df.duplicated(subset = \"text\").sum()}\\n')\n\nprint(f'val_df\\n- Shape: {val_df.shape}\\n- Columns: {list(val_df.columns)}')\nprint(f'- Duplicates: {val_df.duplicated(subset = [\"less_toxic\", \"more_toxic\"]).sum()}!')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:13.574022Z","iopub.execute_input":"2021-11-25T05:06:13.575304Z","iopub.status.idle":"2021-11-25T05:06:14.351683Z","shell.execute_reply.started":"2021-11-25T05:06:13.575238Z","shell.execute_reply":"2021-11-25T05:06:14.350832Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Removing Duplicates in Validation Set\n(Optional) It won't have any effect on our model (Because the improvements matters not the spesific value) but removing duplicate (*less_toxic*, *more_toxic*) pairs might be a good practice.","metadata":{}},{"cell_type":"code","source":"# # Get the dupicate items\n# vals_duplicate_df = val_df[['less_toxic', 'more_toxic']]\n\n# # Drop the duplicate paires except the first occurrence (Remove the worker column as well)\n# val_df = vals_duplicate_df.loc[~vals_duplicate_df.duplicated(keep = 'first')]\n\n# print(f\"- New shape: {val_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:14.354084Z","iopub.execute_input":"2021-11-25T05:06:14.354445Z","iopub.status.idle":"2021-11-25T05:06:14.358969Z","shell.execute_reply.started":"2021-11-25T05:06:14.354390Z","shell.execute_reply":"2021-11-25T05:06:14.358019Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Text-Cleaning","metadata":{}},{"cell_type":"code","source":"# val_df\nval_df['less_toxic'] = val_df['less_toxic'].apply(clean_text)\nval_df['more_toxic'] = val_df['more_toxic'].apply(clean_text)\n\n# test_df\ntest_df['text'] = test_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:06:14.360421Z","iopub.execute_input":"2021-11-25T05:06:14.361222Z","iopub.status.idle":"2021-11-25T05:07:13.423034Z","shell.execute_reply.started":"2021-11-25T05:06:14.361180Z","shell.execute_reply":"2021-11-25T05:07:13.422165Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# jigsaw toxic comment classification challenge\nThe *jigsaw-toxic-comment-train.csv* contains data from *train.csv* and *test.csv* from the *jigsaw-toxic-comment-classification-challenge*. (The test data and their corresponding labels have been merged, then both sets are concatenated)\n\n**NOTE #1**: I will be changing the columns names to match the original dataset columns' names.","metadata":{}},{"cell_type":"code","source":"jtc_df = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv').rename(\n    columns = { 'id': 'comment_id', 'comment_text': 'text'}\n)\n\nprint(f'jtc_df\\n- Shape: {jtc_df.shape}')\nprint(f'- Columns: {list(jtc_df.columns)}')\nprint(f'- Duplicates: {jtc_df.duplicated(\"text\").sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:13.424375Z","iopub.execute_input":"2021-11-25T05:07:13.424606Z","iopub.status.idle":"2021-11-25T05:07:16.912277Z","shell.execute_reply.started":"2021-11-25T05:07:13.424576Z","shell.execute_reply":"2021-11-25T05:07:16.911631Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Combining Toxicity Types","metadata":{}},{"cell_type":"code","source":"# Combine all toxicity levels into one with the same weights set\njtc_df['toxicity'] = sum([jtc_df[type] * coef for type, coef in toxicity_weights.items() if type in jtc_df])\n\n# Standardize toxicity (converts to continues values)\n# jtc_df['toxicity'] = jtc_df['toxicity'] / jtc_df['toxicity'].max()","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:16.913235Z","iopub.execute_input":"2021-11-25T05:07:16.914074Z","iopub.status.idle":"2021-11-25T05:07:16.926578Z","shell.execute_reply.started":"2021-11-25T05:07:16.914036Z","shell.execute_reply":"2021-11-25T05:07:16.925664Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Downsampling & Text-Cleaning\nOur data is heavily unblanaced ([why is that bad?](https://machinelearningmastery.com/what-is-imbalanced-classification/)) and we must fix it. There are a few tricks we can pull off:\n- The weights can be adjusted in a way to try balance out the data (Not recommended - We have enough data for downsampling, don't sacrifice your weights for balancing the data!)\n- Downsampling can drop the portion of data from the problematic side (Most effective)","metadata":{}},{"cell_type":"code","source":"# Downsample\njtc_df = downsample(\n    df = jtc_df,\n    threshold = 0,\n    col = 'toxicity',\n    cutoff_weight = 1.5\n)\nprint(f\"- New shape: {jtc_df.shape}\")\n\n# Clean\njtc_df['text'] = jtc_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:16.927697Z","iopub.execute_input":"2021-11-25T05:07:16.927905Z","iopub.status.idle":"2021-11-25T05:07:56.508659Z","shell.execute_reply.started":"2021-11-25T05:07:16.927878Z","shell.execute_reply":"2021-11-25T05:07:56.507592Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\nExplore fruther the  **jigsaw-toxic-comment-classification-challenge** datast using the following functions:","metadata":{}},{"cell_type":"code","source":"# plot_toxic_types_dist(jtc_df)\n# plot_toxicity_dist(jtc_df)\n# plot_wordcloud(jtc_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:56.510548Z","iopub.execute_input":"2021-11-25T05:07:56.510820Z","iopub.status.idle":"2021-11-25T05:07:56.516975Z","shell.execute_reply.started":"2021-11-25T05:07:56.510786Z","shell.execute_reply":"2021-11-25T05:07:56.516090Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# ruddit jigsaw dataset\nThird dataset used is the *ruddit-jigsaw-dataset* and spesificly the *ruddit_with_text.csv*. There are a few things worth paying attention:\n- Deleted comments are marked as *[deleted]*. Do we keep them? If comment is deleted by the user then it won't have any useful information, but if it's deleted by the community, that would be thoughtful.\n- I shifted the toxicity scores to be between 0 and 1\n\n**NOTE #1**: The *offensiveness_score* is probably different that *toxicity*, but I will rename the column to match the other dataframes.","metadata":{}},{"cell_type":"code","source":"# Select only the columns we need\nrjd_df = pd.read_csv('../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv').rename(\n    columns = {'txt': 'text', 'offensiveness_score': 'toxicity'}\n)[['comment_id', 'text', 'toxicity']]\n\n# Change scale\nrjd_df['toxicity'] = (rjd_df['toxicity'] - rjd_df['toxicity'].min()) / (rjd_df['toxicity'].max() - rjd_df['toxicity'].min()) \n\nprint(f'rjd_df\\n- Shape: {jtc_df.shape}')\nprint(f'- Columns: {list(jtc_df.columns)}')\nprint(f'- Duplicates: {jtc_df.duplicated(\"text\").sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:56.518313Z","iopub.execute_input":"2021-11-25T05:07:56.518738Z","iopub.status.idle":"2021-11-25T05:07:56.676173Z","shell.execute_reply.started":"2021-11-25T05:07:56.518706Z","shell.execute_reply":"2021-11-25T05:07:56.675286Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Removing Invalid Entries & Text-Cleaning\nLooking at the below histogram, there is not much pattern for the deleted comments and I will remove them entirely.","metadata":{}},{"cell_type":"code","source":"# Get duplicates texts\nduplicates = rjd_df['text'].duplicated(keep = 'first')\n\n# Plot distribution of toxicity scores for deleted texts\nplt.figure(figsize = (10, 5))\nplt.hist(rjd_df.loc[duplicates, 'toxicity'])\nplt.show()\n\n# Drop the deleted comments\nrjd_df = rjd_df.loc[rjd_df['text'] != '[deleted]']\nprint(f\"- New shape: {val_df.shape}\")\n\n# Text Cleaing\nrjd_df['text'] = rjd_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:56.677427Z","iopub.execute_input":"2021-11-25T05:07:56.677673Z","iopub.status.idle":"2021-11-25T05:07:58.562265Z","shell.execute_reply.started":"2021-11-25T05:07:56.677643Z","shell.execute_reply":"2021-11-25T05:07:58.561326Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble: Ridge() & TfidfVectorizer()\n## Creating the Pipeline","metadata":{}},{"cell_type":"code","source":"features = FeatureUnion([\n    ('vect', TfidfVectorizer(analyzer = 'char_wb', max_df = 0.5, min_df = 3, ngram_range = (3, 5))),\n])\n\n# Define pipeline\npipe = Pipeline([\n    (\"features\", features),\n    ('ridge', Ridge())\n])","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:58.563890Z","iopub.execute_input":"2021-11-25T05:07:58.564467Z","iopub.status.idle":"2021-11-25T05:07:58.570540Z","shell.execute_reply.started":"2021-11-25T05:07:58.564425Z","shell.execute_reply":"2021-11-25T05:07:58.569584Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Fit, Validate and make Prediction\nI am going to do the followings in each fold:\n1. Train the model\n2. Validate the model (calculate *Accuracy* and *RMSE*)\n3. Predict the model\n\nEach fold uses a subset of the data (the model doesn't see the entire data all at once) and that might cause some problems; However, that's manageable by the number of *folds* we specify that can increase/decrease the amount of data the model works with in each fold.\n\nMoreover, I will have multiple predictions depending on the number of *folds* which I will later sum up. This is a good practice because it reduces possible noises in the data (Although I do shuffle the data).","metadata":{}},{"cell_type":"code","source":"# jtc_df\njtc_lt_preds, jtc_mt_preds, jtc_preds = fit_validate_predict(\n    pipe = pipe,\n    X = np.array(jtc_df['text']),\n    y = np.array(jtc_df['toxicity']),\n    folds = 5\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:07:58.572128Z","iopub.execute_input":"2021-11-25T05:07:58.572639Z","iopub.status.idle":"2021-11-25T05:14:29.397608Z","shell.execute_reply.started":"2021-11-25T05:07:58.572587Z","shell.execute_reply":"2021-11-25T05:14:29.396054Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# rjd_df\nrjd_lt_preds, rjd_mt_preds, rjd_preds = fit_validate_predict(\n    pipe = pipe,\n    X = np.array(rjd_df['text']),\n    y = np.array(rjd_df['toxicity']),\n    folds = 5\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:14:29.399084Z","iopub.status.idle":"2021-11-25T05:14:29.399788Z","shell.execute_reply.started":"2021-11-25T05:14:29.399579Z","shell.execute_reply":"2021-11-25T05:14:29.399602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Grid Search\n# param_grid = {\n#     'vect__max_df': np.concatenate([np.linspace(0, 1, 11), range(1, 10, 1)]),\n#     'vect__min_df': np.concatenate([np.linspace(0, 1, 11), range(1, 10, 1)]),\n#     'vect__ngram_range': [(i, j) for i in range(1, 7) for j in range(1, 7) if j > i]\n# }\n\n# grid = GridSearchCV(pipe, cv = 3, param_grid = param_grid, n_jobs = -1, verbose = 1)\n# grid.fit(X_train,y_train)\n# print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:14:29.400790Z","iopub.status.idle":"2021-11-25T05:14:29.401425Z","shell.execute_reply.started":"2021-11-25T05:14:29.401172Z","shell.execute_reply":"2021-11-25T05:14:29.401200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipe['vect'].get_feature_names()\n\n# sorted(list(zip(pipe['vect'].get_feature_names(), np.round(pipe['ridge'].coef_, 2))), \n#     key = lambda x: x[1], \n#     reverse = True\n# )[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:14:29.402697Z","iopub.status.idle":"2021-11-25T05:14:29.403250Z","shell.execute_reply.started":"2021-11-25T05:14:29.403054Z","shell.execute_reply":"2021-11-25T05:14:29.403076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Modeling\n### Finding optimal weights\n\nI have used *scipy.optimizer* to find the optimal weights (See [documentations](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html)) by brute forcing the possible weights.\n\n**NOTE #1**: When added a new dataset, append the corresponding *lt_preds* and *mt_preds* to the end of *params* list.","metadata":{}},{"cell_type":"code","source":"# parameters\nparams = (\n    [jtc_lt_preds.mean(axis = 1), rjd_lt_preds.mean(axis = 1)],\n    [jtc_mt_preds.mean(axis = 1), rjd_mt_preds.mean(axis = 1)]\n)\n\n# Function which must be minimized\ndef func(x, *param):\n    return -1 * (sum([x[i] * params[0][i] for i in range(len(x))]) < sum([x[i] * params[1][i] for i in range(len(x))])).mean()\n\n# Find optimized weights\nresbrute = scipy.optimize.brute(\n    func,\n    ranges = ([slice(0, 1, 0.01) for _ in range(len(params))]),\n    args = params,\n    full_output = True,\n    finish = None\n)\nprint(f'- Optimal weights: {resbrute[0]}\\n- Global Minimum: {resbrute[1] * -1}')","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:14:29.404300Z","iopub.status.idle":"2021-11-25T05:14:29.404829Z","shell.execute_reply.started":"2021-11-25T05:14:29.404629Z","shell.execute_reply":"2021-11-25T05:14:29.404649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Final Predictions\nWe have the optimal weights and we have the predictions and the final prediction can be calculated using the two.\n\n**NOTE #1**: When added a new dataset, append the corresponding *_preds* to the end of *preds* list.","metadata":{}},{"cell_type":"code","source":"preds = [\n    jtc_preds.mean(axis = 1),\n    rjd_preds.mean(axis = 1)\n]\n\n# Multiply predictions and their corresponding weighs, then sum them up\ny_pred = np.array([preds[i] * resbrute[0][i] for i in range(len(preds))]).sum(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:14:29.405828Z","iopub.status.idle":"2021-11-25T05:14:29.406358Z","shell.execute_reply.started":"2021-11-25T05:14:29.406169Z","shell.execute_reply":"2021-11-25T05:14:29.406189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Submission\nThe predictions are first ranked to get rid of any ties.","metadata":{}},{"cell_type":"code","source":"# Remove ties\ny_pred = scipy.stats.rankdata(y_pred, method = 'ordinal')\n\n# Create submission file\nsubmission_df = pd.DataFrame(data = {\n    'comment_id': test_df['comment_id'],\n    'score': y_pred\n}).to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-25T05:14:29.407361Z","iopub.status.idle":"2021-11-25T05:14:29.407887Z","shell.execute_reply.started":"2021-11-25T05:14:29.407688Z","shell.execute_reply":"2021-11-25T05:14:29.407709Z"},"trusted":true},"execution_count":null,"outputs":[]}]}