{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Rate Severity of Toxic Comments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import re\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom copy import deepcopy\nfrom string import printable\nimport scipy\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import Ridge, ElasticNet","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.739161Z","iopub.execute_input":"2021-11-24T07:29:41.739612Z","iopub.status.idle":"2021-11-24T07:29:41.749278Z","shell.execute_reply.started":"2021-11-24T07:29:41.739572Z","shell.execute_reply":"2021-11-24T07:29:41.748025Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Some constants that we use in several parts of our notebook\nRANDOM_STATE = 201\nSTOPWORDS = set(STOPWORDS)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.751740Z","iopub.execute_input":"2021-11-24T07:29:41.752149Z","iopub.status.idle":"2021-11-24T07:29:41.773615Z","shell.execute_reply.started":"2021-11-24T07:29:41.752097Z","shell.execute_reply":"2021-11-24T07:29:41.772640Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Strategy ⁉\nThere are three datasets introduced in the competition page. I will be using all three to build an Emsemble model. Datasets used in the notebook:\n- [jigsaw-toxic-comment-classification-challenge](https://www.kaggle.com/julian3833/jigsaw-toxic-comment-classification-challenge)\n- [jigsaw-unintended-bias-in-toxicity-classification](https://www.kaggle.com/julian3833/jigsaw-unintended-bias-in-toxicity-classification)","metadata":{}},{"cell_type":"markdown","source":"## Weights\nHere I have defined a dictionary that will map toxicity types to their corresponding weights. These weights are one of the most important parameters in the entire notebook.","metadata":{}},{"cell_type":"code","source":"# Toxicity weights - These weights are later used to combine all toxicity types into one\ntoxicity_weights = {\n    'toxic': 1,\n    'severe_toxic': 2,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 2,\n    'sexual_explicit': 1\n}\n\ntoxicity_types = list(toxicity_weights.keys())","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.775219Z","iopub.execute_input":"2021-11-24T07:29:41.775610Z","iopub.status.idle":"2021-11-24T07:29:41.789559Z","shell.execute_reply.started":"2021-11-24T07:29:41.775578Z","shell.execute_reply":"2021-11-24T07:29:41.788410Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## DownSampling\nA function to downsample a given dataframe by a *threshold*.","metadata":{}},{"cell_type":"code","source":"def downsample(df, threshold, col = 'toxicity', cutoff_weight = 1.5):\n    \n    # Create cutoff\n    cutoff = int((df[col] > threshold).sum() * cutoff_weight)\n\n    # Crate downsampled df\n    downsampled_df = df[df[col] <= threshold].sample(cutoff, random_state = RANDOM_STATE)\n\n    # Concatenate and return the two dataframes\n    return pd.concat([downsampled_df, df[df[col] > threshold]])","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.791259Z","iopub.execute_input":"2021-11-24T07:29:41.792262Z","iopub.status.idle":"2021-11-24T07:29:41.805266Z","shell.execute_reply.started":"2021-11-24T07:29:41.792218Z","shell.execute_reply":"2021-11-24T07:29:41.804293Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Text Cleaning\nAs newer language models and techniques come into play, text-cleaning is becoming less and less necessary and more like an option to include in our proces. But let's not forget that text-cleaning can still be of great importance in many models and scenarios. I have defined a number of functions that will help clean parts of our texts and have later on used a few I believed to be the most helpful of all.","metadata":{}},{"cell_type":"code","source":"HTML_TAG_PATTERN = r\"<.*?>\"\nEMAIL_PATTERN = r'(?:[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\\\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])'\nURL_PATTERN = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n\ndef remove_html_tags(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = HTML_TAG_PATTERN, repl = replace_with, string = string)\n\ndef remove_special_characters(string: str) -> str:\n    return ''.join(filter(lambda x: x in printable, string))\n\ndef remove_urls(string: str, replace_with: str = '') -> str:\n    return re.sub(pattern = URL_PATTERN, repl = replace_with, string = string)\n\ndef remove_emails(string: str, replace_with: str = '') -> str:\n    return re.sub(EMAIL_PATTERN, replace_with, string)\n\ndef remove_repeated_punctuations(string: str) -> str:\n    def replacement(match):\n        match = match.group()\n        return match[0] + (\" \" if \" \" in match else \"\")\n    return re.sub(r'[!\\\"#$%&\\'()*+,\\-.\\/:;<=>?@\\[\\\\\\]^_`{|}~ ]{2,}', replacement, string)\n\n# Removes times and IP addresses\ndef remove_IPs(text):\n    return re.sub(r'(([0-9]+\\.){2,}[0-9]+)', '', text)        # 71.228.77.211\n\ndef remove_times(text):\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2} [a-zA-Z]+,? \\d{4} \\(UTC\\)', '', text)    # 04:09, 11 Jul, 2003  \n    text = re.sub(r'\\d{1,2}:\\d{2},? [a-zA-Z]+ \\d{1,2},? \\d{4} \\(UTC\\)', '', text)    # 16:47, Jul 23, 2004\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{4} [a-zA-Z]+ \\d{1,2},? \\(UTC\\)', '', text)    # 22:07, 2004 Dec 30\n    text = re.sub(r'\\d{1,2} [a-zA-Z]+ \\d{4},? \\d{1,2}:\\d{2} \\(UTC\\)', '', text)      # 29 June 2005 22:08\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2} [a-zA-Z]+,?', '', text)                  # 21:31, 6 April\n    text = re.sub(r'\\d{1,2}:\\d{2},? \\d{1,2},?', '', text)                            # 17:52, 12\n    text = re.sub(r'\\d{1,2}:\\d{1,2}-\\d{1,2}-\\d{1,2}', '', text)                      # 01:05-09-09    \n    text = re.sub(r'\\d{1,2}:\\d{2}', '', text)                                        # 17:52, 12  \n    \n    text = re.sub(r'\\(UTC\\)', '', text)                                              # (UTC)\n    return text\n\n# Replace repeating characters more than 3 times to length of 3\ndef shorten_repeated_patterns(text):\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n    \n    # Add space around repeated characters\n    text = re.sub(r'[ ]{2,}',' ', text).strip()\n    text = re.sub(r'([*!?\\']+)',r' \\1 ', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.840758Z","iopub.execute_input":"2021-11-24T07:29:41.841244Z","iopub.status.idle":"2021-11-24T07:29:41.855743Z","shell.execute_reply.started":"2021-11-24T07:29:41.841209Z","shell.execute_reply":"2021-11-24T07:29:41.854920Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = remove_special_characters(text)\n    text = shorten_repeated_patterns(text)\n    text = remove_html_tags(text)\n    text = remove_emails(text)\n    text = remove_urls(text)\n    text = remove_times(text)\n    rext = remove_IPs\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.857340Z","iopub.execute_input":"2021-11-24T07:29:41.857703Z","iopub.status.idle":"2021-11-24T07:29:41.879756Z","shell.execute_reply.started":"2021-11-24T07:29:41.857672Z","shell.execute_reply":"2021-11-24T07:29:41.878909Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Validation Function\nWe don't have the straight-forward validation data as we normally have, so we must come on with a method to validate our models. I will be using *validation.csv* which has two columns: *less_toxic* and *more_toxic*.\n\nI will predict on each of the two columns and then computer the **RMSE** and the **Accuracy** metrics. This will be done using StratifiedKFold to ensure our results are as accurate as possible. ([Why Stratified?](https://stackoverflow.com/questions/65318931/stratifiedkfold-vs-kfold-in-scikit-learn))\n\n**NOTE #1**: *Accuracy* can be misleading and is not a recommended metrics as our data is strongly unbalanced! ([why?](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/)) That's why I also used the RMSE.","metadata":{}},{"cell_type":"code","source":"# Performs a Stratified K-Fold validation with a given pipeline\ndef kfold_validate(pipe, folds, X, y, less_toxic, more_toxic, verbose = False):\n    \n    accuracies, rmse_scores = [], []\n    skf = StratifiedKFold(\n        n_splits = folds,\n        shuffle = True,\n        random_state = RANDOM_STATE\n    )\n    \n    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        \n        # Copy the original pipeline for each fold (This avoids fitting on the same pipeline multiple times)\n        _pipe = deepcopy(pipe)\n        _pipe.fit(X_train, y_train)\n        \n        # Calculate RMSE\n        rmse_score = mean_squared_error(_pipe.predict(X_val), y_val, squared = False) \n        rmse_scores.append(rmse_score)\n        \n        # Calculate accuracy\n        accuracy = (_pipe.predict(less_toxic) < _pipe.predict(more_toxic)).mean()\n        accuracies.append(accuracy)\n        \n        if verbose:\n            print(f\"FOLD #{fold + 1}: Accuracy: {accuracy}, RMSE: {rmse_score}\")\n        \n    return np.array(accuracies).mean(), np.array(rmse_scores).mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.886105Z","iopub.execute_input":"2021-11-24T07:29:41.886569Z","iopub.status.idle":"2021-11-24T07:29:41.897614Z","shell.execute_reply.started":"2021-11-24T07:29:41.886535Z","shell.execute_reply":"2021-11-24T07:29:41.896496Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Visualization\nSince I'll (probably) be using multiple datasets in this notebook and run pretty much the same analysis over them, I'll define a few methods to avoid code duplication.","metadata":{}},{"cell_type":"code","source":"# Plots number of values for each toxicity level in the given dataframe\ndef plot_toxic_types_dist(df):    \n    fig = plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Categories Count')\n    plt.bar([type for type in toxicity_types if type in jtc_df.columns], [df[type].value_counts()[1] for type in toxicity_types if type in df.columns], label = 'Number of occurrences')\n    plt.legend()\n    plt.show()\n\n\n# Plots the didtribution of values in toxicity columns of the given dataframe\ndef plot_toxicity_dist(df):\n    toxicity_values = df['toxicity'].value_counts()\n    \n    plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Level Distribution')\n    plt.bar(toxicity_values.keys(), toxicity_values.values, color = 'g')\n    plt.show()\n\n# Plots the wordcloud for each toxicity level of the given data frame (Stopwords are removed)\ndef plot_wordcloud(df):\n    wordcloud = WordCloud(stopwords = STOPWORDS)\n    fig, ax = plt.subplots(3, 2, figsize = (20, 10))\n\n    i = 0\n    for row in ax:\n        for col in row:        \n            wordcloud.generate(' '.join(df.loc[df[toxicity_types[i]] != 0, 'text'].tolist()))\n            col.set_title(toxicity_types[i])        \n            col.imshow(wordcloud)        \n            col.axis(\"off\")\n            i += 1\n    plt.tight_layout(pad = 0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:41.998067Z","iopub.execute_input":"2021-11-24T07:29:41.998679Z","iopub.status.idle":"2021-11-24T07:29:42.013108Z","shell.execute_reply.started":"2021-11-24T07:29:41.998627Z","shell.execute_reply":"2021-11-24T07:29:42.010795Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Jigsaw Rate Severity of Toxic Comments\nThis is our original dataset for the competition. The columns are:\n- *comment_to_score.csv*: The dataset that is used for the final predictions.\n- *validation_data.csv*: The dataset that is used to validate the models.\n- *sample_submission.csv*: A sample submission file.\n\n**STEP #1**: Our validation contain duplicate (*less_toxic*, *more_toxic*) pairs. This won't be problematic for our metrics (metric improvement matters not its specific value), but I will remove the duplicates anyway.\n\n**STEP #2**: I will remove the *email addresses*, *html tags*, *URLs*, *times* and *IP addresses*.","metadata":{}},{"cell_type":"code","source":"val_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntest_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\nprint(f'test_df\\n- Shape: {test_df.shape}\\n- Columns: {list(test_df.columns)}')\nprint(f'- Duplicates: {test_df.duplicated(subset = \"text\").sum()}\\n')\n\nprint(f'val_df\\n- Shape: {val_df.shape}\\n- Columns: {list(val_df.columns)}')\nprint(f'- Duplicates: {val_df.duplicated(subset = [\"less_toxic\", \"more_toxic\"]).sum()}!')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:42.015994Z","iopub.execute_input":"2021-11-24T07:29:42.016867Z","iopub.status.idle":"2021-11-24T07:29:42.502532Z","shell.execute_reply.started":"2021-11-24T07:29:42.016798Z","shell.execute_reply":"2021-11-24T07:29:42.501558Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# # Get the dupicate items\n# vals_duplicate_df = val_df[['less_toxic', 'more_toxic']]\n\n# # Drop the duplicate paires except the first occurrence (Remove the worker column as well)\n# val_df = vals_duplicate_df.loc[~vals_duplicate_df.duplicated(keep = 'first')]\n\n# print(f\"- New shape: {val_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:42.503909Z","iopub.execute_input":"2021-11-24T07:29:42.504886Z","iopub.status.idle":"2021-11-24T07:29:42.510050Z","shell.execute_reply.started":"2021-11-24T07:29:42.504832Z","shell.execute_reply":"2021-11-24T07:29:42.508667Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# val_df\nval_df['less_toxic'] = val_df['less_toxic'].apply(clean_text)\nval_df['more_toxic'] = val_df['more_toxic'].apply(clean_text)\n\n# test_df\ntest_df['text'] = test_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:29:42.513681Z","iopub.execute_input":"2021-11-24T07:29:42.514344Z","iopub.status.idle":"2021-11-24T07:30:23.054133Z","shell.execute_reply.started":"2021-11-24T07:29:42.514300Z","shell.execute_reply":"2021-11-24T07:30:23.052465Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## jigsaw toxic comment classification challenge\nThe *jigsaw-toxic-comment-train.csv* contains data from *train.csv* and *test.csv* from the *jigsaw-toxic-comment-classification-challenge*. (The test data and their corresponding labels have been merged, then both sets are concatenated)\n\n**NOTE #1**: I will be changing the columns names to match the original dataset columns' names.","metadata":{}},{"cell_type":"code","source":"jtc_df = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv').rename(\n    columns = { 'id': 'comment_id', 'comment_text': 'text'}\n)\n\nprint(f'jtc_df\\n- Shape: {jtc_df.shape}')\nprint(f'- Columns: {list(jtc_df.columns)}')\nprint(f'- Duplicates: {jtc_df.duplicated(\"text\").sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:30:23.056573Z","iopub.execute_input":"2021-11-24T07:30:23.057027Z","iopub.status.idle":"2021-11-24T07:30:25.081532Z","shell.execute_reply.started":"2021-11-24T07:30:23.056985Z","shell.execute_reply":"2021-11-24T07:30:25.080571Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Combine all toxicity levels into one with the same weights set\njtc_df['toxicity'] = sum([jtc_df[type] * coef for type, coef in toxicity_weights.items() if type in jtc_df])\n\n# Standardize toxicity\n# jtc_df['toxicity'] = jtc_df['toxicity'] / jtc_df['toxicity'].max()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:30:25.082979Z","iopub.execute_input":"2021-11-24T07:30:25.083260Z","iopub.status.idle":"2021-11-24T07:30:25.305268Z","shell.execute_reply.started":"2021-11-24T07:30:25.083227Z","shell.execute_reply":"2021-11-24T07:30:25.304214Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Downsampling & Text-Cleaning\nOur data is heavily unblanaced ([why is that bad?](https://machinelearningmastery.com/what-is-imbalanced-classification/)) and we must fix it. There are a few tricks we can pull off:\n- The weights can be adjusted in a way to try balance out the data (Not recommended - We have enough data for downsampling, don't sacrifice your weights for balancing the data!)\n- Downsampling can drop the portion of data from the problematic side (Most effective)","metadata":{}},{"cell_type":"code","source":"# Downsample\njtc_df = downsample(\n    df = jtc_df,\n    threshold = 0,\n    col = 'toxicity',\n    cutoff_weight = 1.5\n)\nprint(f\"- New shape: {jtc_df.shape}\")\n\n# Clean\njtc_df['text'] = jtc_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:30:25.307005Z","iopub.execute_input":"2021-11-24T07:30:25.307508Z","iopub.status.idle":"2021-11-24T07:30:55.043472Z","shell.execute_reply.started":"2021-11-24T07:30:25.307279Z","shell.execute_reply":"2021-11-24T07:30:55.042753Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\nExplore fruther the  **jigsaw-toxic-comment-classification-challenge** datast using the following functions:","metadata":{}},{"cell_type":"code","source":"# plot_toxic_types_dist(jtc_df)\n# plot_toxicity_dist(jtc_df)\n# plot_wordcloud(jtc_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:30:55.045563Z","iopub.execute_input":"2021-11-24T07:30:55.046056Z","iopub.status.idle":"2021-11-24T07:30:55.051918Z","shell.execute_reply.started":"2021-11-24T07:30:55.046004Z","shell.execute_reply":"2021-11-24T07:30:55.050982Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Pipeline","metadata":{}},{"cell_type":"code","source":"X_train = jtc_df['text']\ny_train = jtc_df['toxicity']\nX_test = test_df['text']","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:30:55.053577Z","iopub.execute_input":"2021-11-24T07:30:55.054445Z","iopub.status.idle":"2021-11-24T07:30:55.073822Z","shell.execute_reply.started":"2021-11-24T07:30:55.054387Z","shell.execute_reply":"2021-11-24T07:30:55.072766Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# pipe = Pipeline([\n#     ('vect', TfidfVectorizer(analyzer = 'char_wb')),\n#     ('ridge', Ridge())\n# ])\n\n# # CV Cross Validation\n# cv_scores = cross_val_score(\n#     estimator = pipe,\n#     X = X_train,\n#     y = y_train,\n#     cv = 5,\n#     n_jobs = -1\n# )\n# print(f\"Average CV Score: {cv_scores.mean()}\")\n\n# # Grid Search\n# param_grid = {\n#     'vect__max_df': np.concatenate([np.linspace(0, 1, 11), range(1, 10, 1)]),\n#     'vect__min_df': np.concatenate([np.linspace(0, 1, 11), range(1, 10, 1)]),\n#     'vect__ngram_range': [(i, j) for i in range(1, 7) for j in range(1, 7) if j > i]\n# }\n\n# grid = GridSearchCV(pipe, cv = 3, param_grid = param_grid, n_jobs = -1, verbose = 1)\n# grid.fit(X_train,y_train)\n# print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:30:55.077136Z","iopub.execute_input":"2021-11-24T07:30:55.077593Z","iopub.status.idle":"2021-11-24T07:30:55.088740Z","shell.execute_reply.started":"2021-11-24T07:30:55.077528Z","shell.execute_reply":"2021-11-24T07:30:55.087698Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Define pipeline\npipe = Pipeline([\n    ('vect', TfidfVectorizer(analyzer = 'char_wb', max_df = 0.5, min_df = 3, ngram_range = (3, 5))),\n    ('ridge', Ridge())\n])\n\n# Validate (Pipeline must not be fitted!)\nacc_mean, rmse_mean = kfold_validate(\n    pipe = pipe,\n    folds = 7,\n    X = np.array(X_train),\n    y = np.array(y_train),\n    less_toxic = val_df['less_toxic'],\n    more_toxic = val_df['more_toxic'],\n    verbose = True\n)\nprint(f\"Mean Accuracy: {acc_mean}\\nMean RMSE: {rmse_mean}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:33:41.974875Z","iopub.execute_input":"2021-11-24T07:33:41.975629Z","iopub.status.idle":"2021-11-24T07:47:47.546015Z","shell.execute_reply.started":"2021-11-24T07:33:41.975583Z","shell.execute_reply":"2021-11-24T07:47:47.544894Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# pipe['vect'].get_feature_names()\n\n# sorted(list(zip(pipe['vect'].get_feature_names(), np.round(pipe['ridge'].coef_, 2))), \n#     key = lambda x: x[1], \n#     reverse = True\n# )[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:47:47.547828Z","iopub.execute_input":"2021-11-24T07:47:47.548100Z","iopub.status.idle":"2021-11-24T07:47:47.553289Z","shell.execute_reply.started":"2021-11-24T07:47:47.548068Z","shell.execute_reply":"2021-11-24T07:47:47.552235Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Submission","metadata":{}},{"cell_type":"code","source":"# Train the pipeline\npipe.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipe.predict(X_test)\n\n# Rank the predictions to avoid ties\ny_pred = scipy.stats.rankdata(y_pred, method = 'ordinal')\n\n# Create submission file\nsubmission_df = pd.DataFrame(data = {\n    'comment_id': test_df['comment_id'],\n    'score': y_pred\n}).to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:47:47.554663Z","iopub.execute_input":"2021-11-24T07:47:47.554902Z","iopub.status.idle":"2021-11-24T07:48:57.870947Z","shell.execute_reply.started":"2021-11-24T07:47:47.554872Z","shell.execute_reply":"2021-11-24T07:48:57.869771Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}