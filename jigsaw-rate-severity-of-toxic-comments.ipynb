{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Rate Severity of Toxic Comments\nSpecial thanks to:\n- [Jigsaw - Incredibly Simple Naive Bayes [0.768]](https://www.kaggle.com/julian3833/jigsaw-incredibly-simple-naive-bayes-0-768)\n- [JRSoTC - RidgeRegression (ensemble of 3)](https://www.kaggle.com/steubk/jrsotc-ridgeregression-ensemble-of-3/notebook)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom copy import deepcopy\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nstopwords = set(STOPWORDS)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import Ridge","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:16.960967Z","iopub.execute_input":"2021-11-21T21:51:16.961294Z","iopub.status.idle":"2021-11-21T21:51:18.171392Z","shell.execute_reply.started":"2021-11-21T21:51:16.961264Z","shell.execute_reply":"2021-11-21T21:51:18.170269Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"RANDOM_STATE = 201","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:18.173365Z","iopub.execute_input":"2021-11-21T21:51:18.173640Z","iopub.status.idle":"2021-11-21T21:51:18.178234Z","shell.execute_reply.started":"2021-11-21T21:51:18.173609Z","shell.execute_reply":"2021-11-21T21:51:18.177442Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Strategy ‚Åâ\nThere are three datasets introduced in the competition page. I will be using all three to build an Emsemble model. Datasets used in the notebook:\n- [jigsaw-toxic-comment-classification-challenge](https://www.kaggle.com/julian3833/jigsaw-toxic-comment-classification-challenge)\n- [jigsaw-unintended-bias-in-toxicity-classification](https://www.kaggle.com/julian3833/jigsaw-unintended-bias-in-toxicity-classification)","metadata":{}},{"cell_type":"code","source":"# Toxicity coefficients - These weights are used to combine all toxicity levels into one\ntoxicity_coefs = {\n    'toxic': 1,\n    'severe_toxic': 2,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 2,\n    'sexual_explicit': 1\n}\n\ntoxicity_types = list(toxicity_coefs.keys())","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:18.179470Z","iopub.execute_input":"2021-11-21T21:51:18.180315Z","iopub.status.idle":"2021-11-21T21:51:18.190680Z","shell.execute_reply.started":"2021-11-21T21:51:18.180269Z","shell.execute_reply":"2021-11-21T21:51:18.190025Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Validation: Defining our Validation Method\nWe need to validate our model to tracks its performance. In the process, we use *validation_data.csv* as our validation set. I used *RMSE* and *Accuracy* as my metrics.\n\n**NOTE #1**: *Accuracy* is not a recommended metrics as our data is strongly unbalanced! ([See why](https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/))","metadata":{}},{"cell_type":"code","source":"# Performs a Stratified K-Fold validation using the given pipeline\ndef kfold_validate(pipe, folds, X, y, less_toxic, more_toxic, verbose = False):\n    skf = StratifiedKFold(n_splits = folds, shuffle = True, random_state = RANDOM_STATE)\n    accuracies, rmse_scores = [], []    \n    \n    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        \n        # Fit the pipeline (Re-copy the pipeline to avoid fitting on the same one!)\n        _pipe = deepcopy(pipe)\n        _pipe.fit(X_train, y_train)\n        \n        # Calculate RMSE\n        rmse_score = mean_squared_error(_pipe.predict(X_val), y_val, squared = False) \n        rmse_scores.append(rmse_score)\n        \n        # Calculate accuracy\n        prob_1 = _pipe.predict(less_toxic)\n        prob_2 = _pipe.predict(more_toxic)\n        accuracy = (prob_1 < prob_2).mean()\n        accuracies.append(accuracy)\n        \n        if verbose:\n            print(f\"FOLD #{fold + 1}: Accuracy: {accuracy}, RMSE: {rmse_score}\")\n        \n    return np.array(accuracies).mean(), np.array(rmse_scores).mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:18.192682Z","iopub.execute_input":"2021-11-21T21:51:18.193200Z","iopub.status.idle":"2021-11-21T21:51:18.204877Z","shell.execute_reply.started":"2021-11-21T21:51:18.193155Z","shell.execute_reply":"2021-11-21T21:51:18.204109Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization: Plotting the Data\nSince we pretty much do the same analysis all datasets, plots are converted into functions to avoid further duplication.","metadata":{}},{"cell_type":"code","source":"# Plots the didtribution of values in toxicity columns of the given dataframe\ndef plot_toxicity_dist(df):\n    toxicity_values = df['toxicity'].value_counts()\n    \n    plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Level Distribution')\n    plt.bar(toxicity_values.keys(), toxicity_values.values, color = 'g')\n    plt.show()\n\n\n# Plots number of values for each toxicity level in the given dataframe\ndef plot_toxic_types_dist(df):    \n    fig = plt.figure(figsize = (20, 5))\n    plt.title('Toxicity Categories Count')\n    plt.bar([type for type in toxicity_types if type in jtc_df.columns], [df[type].value_counts()[1] for type in toxicity_types if type in df.columns], label = 'Number of occurrences')\n    plt.legend()\n    plt.show()\n\n\n# Plots the wordcloud for each toxicity level of the given data frame (Stopwords are removed)\ndef plot_wordcloud(df):\n    wordcloud = WordCloud(stopwords = stopwords)\n    fig, ax = plt.subplots(3, 2, figsize = (20, 10))\n\n    i = 0\n    for row in ax:\n        for col in row:        \n            wordcloud.generate(' '.join(df.loc[df[toxicity_types[i]] != 0, 'text'].tolist()))\n            col.set_title(toxicity_types[i])        \n            col.imshow(wordcloud)        \n            col.axis(\"off\")\n            i += 1\n    plt.tight_layout(pad = 0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:18.206333Z","iopub.execute_input":"2021-11-21T21:51:18.206890Z","iopub.status.idle":"2021-11-21T21:51:18.219265Z","shell.execute_reply.started":"2021-11-21T21:51:18.206834Z","shell.execute_reply":"2021-11-21T21:51:18.218336Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Jigsaw Rate Severity of Toxic Comments\nThis is our original dataset for the competition, let's take a look:\n- *comment_to_score.csv*: This is the final dataset that we have to make predictions on.\n- *validation_data.csv*: The dataset that will help validate our model.\n- *sample_submission.csv*: A sample submission file.","metadata":{}},{"cell_type":"code","source":"val_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntest_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\nprint(f'test_df\\n- Shape: {test_df.shape}\\n- Columns: {list(test_df.columns)}\\n')\nprint(f'Duplicated texts: {test_df.duplicated(\"text\").sum()}')\n\nprint(f'val_df\\n- Shape: {val_df.shape}\\n- Columns: {list(val_df.columns)}\\n')\nprint(f'Duplicated texts: {val_df.duplicated(\"text\").sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:18.221578Z","iopub.execute_input":"2021-11-21T21:51:18.221946Z","iopub.status.idle":"2021-11-21T21:51:18.854551Z","shell.execute_reply.started":"2021-11-21T21:51:18.221906Z","shell.execute_reply":"2021-11-21T21:51:18.853599Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## jigsaw toxic comment classification challenge\nThis is the second dataset in the notebook. Let's take a look at what we have:\n- *train.csv*: The training data.\n- *test.csv*: The test data used for final prediction\n- *test_labels.csv*: The actual answers for the *test.csv*.\n- *sample_submission.csv*: A sample submission file.\n\n**NOTE #1**: Since the actual test labels are published, I will be using them to increase the number of training data.\n\n**NOTE #2**: I will be changing the columns names to match the original dataset columns' names. (This applies to all used datasets)","metadata":{}},{"cell_type":"markdown","source":"#### Loading the training data: *train.csv*","metadata":{}},{"cell_type":"code","source":"# Renaming the columns will help avoid any complications\njtc_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv').rename(\n    columns = { 'id': 'comment_id', 'comment_text': 'text'}\n)\n\njtc_test_df = pd.merge(\n    pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv'),\n    pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv'),\n    on = 'id',\n    how = 'outer'\n).rename(columns = {'id': 'comment_id', 'comment_text': 'text'})\n\n# Add to the training data\njtc_df = pd.concat([jtc_df, jtc_test_df])\n\nprint(f'- Shape: {jtc_df.shape}\\n- Columns: {list(jtc_df.columns)}\\n')\nprint(f'Duplicated texts: {jtc_df.duplicated(\"text\").sum()}')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:18.855972Z","iopub.execute_input":"2021-11-21T21:51:18.856218Z","iopub.status.idle":"2021-11-21T21:51:22.908729Z","shell.execute_reply.started":"2021-11-21T21:51:18.856187Z","shell.execute_reply":"2021-11-21T21:51:22.907534Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Expanding the Data: Adding *test.csv* and *test_labels.csv*","metadata":{}},{"cell_type":"code","source":"# Combine all toxicity levels into one with the same weights set\njtc_df['toxicity'] = sum([jtc_df[type] * coef for type, coef in toxicity_coefs.items() if type in jtc_df])\n\n# Filter the ones with negative toxicity (They are invalid)\njtc_df = jtc_df.loc[jtc_df['toxicity'] >= 0]\n\nprint(f\"- New shape: {jtc_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:22.910299Z","iopub.execute_input":"2021-11-21T21:51:22.910533Z","iopub.status.idle":"2021-11-21T21:51:22.967239Z","shell.execute_reply.started":"2021-11-21T21:51:22.910504Z","shell.execute_reply":"2021-11-21T21:51:22.966114Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Downsampling\nOur data is heavily unblanaced ([See why that's bad](https://machinelearningmastery.com/what-is-imbalanced-classification/)) and we must fix it. There are a few tricks we can pull off but down-sampling is the best way to go.","metadata":{}},{"cell_type":"code","source":"# Cutoff & threshold\nthreshold = 0\ncutoff = (jtc_df['toxicity'] > threshold).sum()\ncutoff_coef = 1.5\n\n# Downsample non-toxic comments\njtc_non_toxic_df = jtc_df.loc[jtc_df['toxicity'] <= threshold].sample(int(cutoff * cutoff_coef), random_state = RANDOM_STATE)\n\n# Concatenate the two dataframes\njtc_df = pd.concat([jtc_non_toxic_df, jtc_df[jtc_df['toxicity'] > threshold]])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:22.968873Z","iopub.execute_input":"2021-11-21T21:51:22.969462Z","iopub.status.idle":"2021-11-21T21:51:23.040406Z","shell.execute_reply.started":"2021-11-21T21:51:22.969406Z","shell.execute_reply":"2021-11-21T21:51:23.039221Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### EDA: Exploratory Data Analysis\nOur data from **jigsaw-toxic-comment-classification-challenge** is ready to be fed into a model and then prediction, which results in a clean 77% score on the submission. I did further explorations on the data but to keep things short, they are commented out below. (Run in seperate cells)","metadata":{}},{"cell_type":"code","source":"# plot_toxic_types_dist(jtc_df)\n# plot_toxicity_dist(jtc_df)\n# plot_wordcloud(jtc_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:51:23.047073Z","iopub.execute_input":"2021-11-21T21:51:23.047663Z","iopub.status.idle":"2021-11-21T21:51:23.052576Z","shell.execute_reply.started":"2021-11-21T21:51:23.047610Z","shell.execute_reply":"2021-11-21T21:51:23.051931Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## jigsaw unintended bias in toxicity classification\nOur third dataset has many dataframes but luckily *all_data.csv* contains them all.\n\n**NOTE #1**: The *toxicity_annotator_count* feature can be used to remove the comments with very few annotators. I'll remove the ones with less than 10 annotators.\n\n**NOTE #2**: The *sexual_explicit* feature is new, but keeping it might be a good idea, why?","metadata":{}},{"cell_type":"code","source":"jutc_features_to_select = ['comment_id', 'text', 'toxic', 'severe_toxic', 'obscene', 'insult', 'identity_hate', 'sexual_explicit', 'toxicity_annotator_count']\n\n# Load and rename columns\njutc_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/all_data.csv').rename(\n    columns = {\n        'id': 'comment_id',\n        'comment_text': 'text',\n        'identity_attack': 'identity_hate',\n        'toxicity': 'toxic',\n        'severe_toxicity': 'severe_toxic'\n    }\n)\n\n# Filter annonators and select only the features we need\njutc_df = jutc_df.loc[jutc_df['toxicity_annotator_count'] > 5, jutc_features_to_select]\n\nprint(f'- Shape: {jutc_df.shape}\\n- Columns: {list(jutc_df.columns)}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:57:50.063664Z","iopub.execute_input":"2021-11-21T21:57:50.064944Z","iopub.status.idle":"2021-11-21T21:58:12.448298Z","shell.execute_reply.started":"2021-11-21T21:57:50.064891Z","shell.execute_reply":"2021-11-21T21:58:12.447416Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Calculate toxicity\njutc_df['toxicity'] = jutc_df[['severe_toxic', 'obscene', 'insult', 'identity_hate', 'sexual_explicit']].sum(axis = 1)\n\njutc_df['toxicity'] = jutc_df.apply(lambda x: x[\"toxic\"] if x[\"toxic\"] <= 0.5 else x[\"toxicity\"], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:58:55.523230Z","iopub.execute_input":"2021-11-21T21:58:55.523898Z","iopub.status.idle":"2021-11-21T21:59:07.265571Z","shell.execute_reply.started":"2021-11-21T21:58:55.523854Z","shell.execute_reply":"2021-11-21T21:59:07.264341Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#### Downsampling\nThis dataset is more balanced that the previous one, but still requires dow-sampling.","metadata":{}},{"cell_type":"code","source":"# Cutoff and threshold\nthreshold = 0.5\ncutoff = (jutc_df['toxicity'] > threshold).sum()\ncutoff_coef = 1.5\n\n# Downsample non-toxic comments\njutc_non_toxic_df = jutc_df[jutc_df['toxicity'] <= threshold].sample(int(cutoff * cutoff_coef), random_state = RANDOM_STATE)\n\n# Concatenate the two dataframes\njutc_df = pd.concat([jutc_non_toxic_df, jutc_df[jutc_df['toxicity'] > threshold]])\n\nprint(f'- Shape: {jutc_df.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T22:00:47.348594Z","iopub.execute_input":"2021-11-21T22:00:47.349580Z","iopub.status.idle":"2021-11-21T22:00:47.497228Z","shell.execute_reply.started":"2021-11-21T22:00:47.349529Z","shell.execute_reply":"2021-11-21T22:00:47.496256Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Convert to discrete valeus (instead of continuous)\njutc_df['toxicity'] = (np.round(jutc_df['toxicity'], decimals = 1) * 10).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:52:05.675890Z","iopub.execute_input":"2021-11-21T21:52:05.676597Z","iopub.status.idle":"2021-11-21T21:52:05.699367Z","shell.execute_reply.started":"2021-11-21T21:52:05.676543Z","shell.execute_reply":"2021-11-21T21:52:05.698645Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### EDA: Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# plot_toxicity_dist(jutc_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T21:52:05.700871Z","iopub.execute_input":"2021-11-21T21:52:05.701193Z","iopub.status.idle":"2021-11-21T21:52:05.711750Z","shell.execute_reply.started":"2021-11-21T21:52:05.701145Z","shell.execute_reply":"2021-11-21T21:52:05.711053Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Text Cleaning\nAs language models improve, text-cleaning is becoming less necessary, but that's not the case for all models. My strategy is to start simple and test some cleaning methods to see if they help the model or not.","metadata":{"execution":{"iopub.status.busy":"2021-11-16T20:37:56.996117Z","iopub.execute_input":"2021-11-16T20:37:56.99644Z","iopub.status.idle":"2021-11-16T20:37:57.002838Z","shell.execute_reply.started":"2021-11-16T20:37:56.996406Z","shell.execute_reply":"2021-11-16T20:37:57.001704Z"}}},{"cell_type":"markdown","source":"### Modeling: Creating the Pipeline","metadata":{}},{"cell_type":"code","source":"train_X = jtc_df['text']\ntrain_y = jtc_df['toxicity']\ntest_X = test_df['text']\n\n# train_X = jutc_df['text']\n# train_y = jutc_df['toxicity']\n# test_X = test_df['text']","metadata":{"execution":{"iopub.status.busy":"2021-11-21T22:04:24.929140Z","iopub.execute_input":"2021-11-21T22:04:24.929516Z","iopub.status.idle":"2021-11-21T22:04:24.935693Z","shell.execute_reply.started":"2021-11-21T22:04:24.929480Z","shell.execute_reply":"2021-11-21T22:04:24.935053Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Define pipeline\npipe = Pipeline([\n    ('vectorizer', TfidfVectorizer(analyzer = 'char_wb', max_df = 0.5, min_df = 3, ngram_range = (4, 6))),\n    ('model', Ridge())\n])\n\n# Validate (Pipeline must not be fitted!)\nacc_mean, rmse_mean = kfold_validate(\n    pipe = pipe,\n    folds = 5,\n    X = np.array(train_X),\n    y = np.array(train_y),\n    less_toxic = val_df['less_toxic'],\n    more_toxic = val_df['more_toxic'],\n    verbose = True\n)\nprint(f\"Mean Accuracy: {acc_mean}\\nMean RMSE: {rmse_mean}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T22:04:25.302973Z","iopub.execute_input":"2021-11-21T22:04:25.304059Z","iopub.status.idle":"2021-11-21T22:13:05.223751Z","shell.execute_reply.started":"2021-11-21T22:04:25.304015Z","shell.execute_reply":"2021-11-21T22:13:05.222511Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Submission","metadata":{}},{"cell_type":"code","source":"# Train the pipeline\npipe.fit(train_X, train_y)\n\n# Make predictions\ny_pred = pipe.predict(test_X)\n\n# Create submission file\nsubmission_df = pd.DataFrame(data = {\n    'comment_id': test_df['comment_id'],\n    'score': y_pred\n}).to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T22:21:46.911387Z","iopub.execute_input":"2021-11-21T22:21:46.911785Z","iopub.status.idle":"2021-11-21T22:21:46.917529Z","shell.execute_reply.started":"2021-11-21T22:21:46.911749Z","shell.execute_reply":"2021-11-21T22:21:46.916376Z"},"trusted":true},"execution_count":42,"outputs":[]}]}