{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Jigsaw Rate Severity of Toxic Comments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom string import printable, punctuation\nfrom itertools import groupby\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nstopwords = set(STOPWORDS)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:36:54.301635Z","iopub.execute_input":"2021-11-20T16:36:54.301957Z","iopub.status.idle":"2021-11-20T16:36:54.309184Z","shell.execute_reply.started":"2021-11-20T16:36:54.301922Z","shell.execute_reply":"2021-11-20T16:36:54.308310Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## EDA: Love at First Sight ðŸ‘‰ðŸ‘ˆ\nI have to be comfortable with the data I'm working with in order to understand it. So let's take a look at what we have:\n- **comments_to_score.csv**: This is our test set which we use for the final prediction. We must score the toxicity level of each comment.\n- **sample_submission.csv**: Just a sample submission file.\n- **validation_data.csv**: Pair of comments that are used for model validation. Our validation is mainly based this dataset (However we could use more data from the following dataset..)\n\nYou probably have wondered, where the hell is the training data??? Don't worry, they are in the previous competition [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data). We have:\n- **sample_submission.csv**: Yeah, you guessed it.\n- **test.csv**: Dataset used for model prediction.\n- **test_labels.csv**: The actual answers for the *test.csv*.\n- **train.csv**: Yeeey! our training data!!!\n\nIn the second dataset, we know the actual answers to the *test.csv*. So, we could combine that with our *train.csv* to **extend our training data** or used it for **validation** (maybe a different validation from what we have in the original dataset). It's really up to you what you do with them, but I suggest trying them both!","metadata":{}},{"cell_type":"code","source":"# Original competition data\nval_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ntest_df = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\n\n# Previous competition data - Adjusted the column names for more clarity\ntrain_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntrain_df.rename(columns = {'id': 'comment_id','comment_text': 'text'}, inplace = True)\n\nprint(f'train_df\\n- Shape: {train_df.shape}\\n- Columns: {list(train_df.columns)}\\n')\nprint(f'test_df\\n- Shape: {test_df.shape}\\n- Columns: {list(test_df.columns)}\\n')\nprint(f'val_df\\n- Shape: {val_df.shape}\\n- Columns: {list(val_df.columns)}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:36:54.342311Z","iopub.execute_input":"2021-11-20T16:36:54.342667Z","iopub.status.idle":"2021-11-20T16:36:55.903078Z","shell.execute_reply.started":"2021-11-20T16:36:54.342629Z","shell.execute_reply":"2021-11-20T16:36:55.902017Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Good, we know what we've got. Time to do some ***Exploratory Data Analysis*** and ***Feature Engineering***...\n### Exploratory Data Analysis (EDA)\nSince the training data in divided into 6 categories, we must analyze them seperatly and see (for example) what makes *toxic* comment different from an *insult*. We do the followings:\n- First, plot the distribution of all categories to see the occurrence of each category.\n- It wouldn't hurt to use a WordCloud to see the important words for each category.","metadata":{}},{"cell_type":"code","source":"categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nfig = plt.figure(figsize = (20, 5))\nplt.title('Toxicity Categories Count')\nplt.bar(categories, [train_df[cat].value_counts()[1] for cat in categories], label = 'Number of occurrences')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:36:55.904821Z","iopub.execute_input":"2021-11-20T16:36:55.905116Z","iopub.status.idle":"2021-11-20T16:36:56.130958Z","shell.execute_reply.started":"2021-11-20T16:36:55.905079Z","shell.execute_reply":"2021-11-20T16:36:56.130015Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(stopwords = stopwords)\nfig, ax = plt.subplots(3, 2, figsize = (20, 10))\n\ni = 0\nfor row in ax:\n    for col in row:        \n        wordcloud.generate(' '.join(train_df.loc[train_df[categories[i]] == 1, 'text'].tolist()))\n        col.set_title(categories[i])        \n        col.imshow(wordcloud)        \n        col.axis(\"off\")\n        i += 1\nplt.tight_layout(pad = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:36:56.132671Z","iopub.execute_input":"2021-11-20T16:36:56.133032Z","iopub.status.idle":"2021-11-20T16:37:06.958733Z","shell.execute_reply.started":"2021-11-20T16:36:56.132986Z","shell.execute_reply":"2021-11-20T16:37:06.957546Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"The two columns *idenity_hate* and *threat* are pretty straight forward according to our results because they have their own vocabulary (: But others share common words.\n### Feature Engineering\nWe need to somehow combine these 6 categories into one. (For instance) looking at the categories, *severe_toxic* and *thread* cannot be treated equally. Same applies to all categories, so we need to assign weights for each category. **It is extremely important what weight you use for each category!!!**","metadata":{}},{"cell_type":"code","source":"toxicity_coefs = {\n    'toxic': 1,\n    'severe_toxic': 2,\n    'obscene': 1,\n    'threat': 1,\n    'insult': 1,\n    'identity_hate': 2\n}\ntrain_df['toxicity'] = sum([train_df[type] * coef for type, coef in toxicity_coefs.items()])\n\nprint(f\"Number of distinct values: {len(train_df['toxicity'].unique())}\")\nprint(f\"Toxicity Mean: {train_df['toxicity'].mean()}\")\nprint(f\"Standard Ddeviation: {train_df['toxicity'].std()}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:37:06.960996Z","iopub.execute_input":"2021-11-20T16:37:06.961798Z","iopub.status.idle":"2021-11-20T16:37:06.984675Z","shell.execute_reply.started":"2021-11-20T16:37:06.961748Z","shell.execute_reply":"2021-11-20T16:37:06.983811Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"toxicity_values = train_df['toxicity'].value_counts()\n\nplt.figure(figsize = (20, 5))\nplt.title('Toxicity Level Distribution')\nplt.bar(toxicity_values.keys(), toxicity_values.values, color = 'g')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:29:35.578382Z","iopub.execute_input":"2021-11-20T16:29:35.578992Z","iopub.status.idle":"2021-11-20T16:29:35.783707Z","shell.execute_reply.started":"2021-11-20T16:29:35.578955Z","shell.execute_reply":"2021-11-20T16:29:35.782592Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Balancing our data\nAs you saw in the above plot, our set of weights matter alot. We should choose a set which both **makes sense** and it is **as balanced as it can be**.\n\nFor instance if we set all weights to 1, it would violate both our rules: weights won't make sense as *toxic* isn't equal to *threat* (obviously!) and also will create a very unbalanced results where we would have more than 140k comments with 0 toxicity but fewer than 10k comments for other toxicity scores.\n\nInbalanced data can be very problematic. Yes, we can pull a few tricks but at a cost. There are three approches to solve this problem:\n- Our most reliable option is that we can (and we will) downsample our data.\n- Our second most reliable option is ***NOT*** our weights. Yes, ***NOT***! Because we have many zeros and there is absolutely nothing our weights can do about them.\n- Our third reliable option is to use *test.csv* and *test_labels.csv* from the previous competition to add to our training data. Yes, this will make matters worse in terms of balance (because *test.csv* itself is unbalanced!), but at least we have more data for each toxicity level. Then we could downsample and we should lose less data.\n- We could also smooth our *toxicity score* and merge a few values as shown below..","metadata":{}},{"cell_type":"markdown","source":"### Increasing our Training Data","metadata":{}},{"cell_type":"code","source":"# Merge two dataframes test.csv & test_labels.csv\ntmp_df = pd.merge(\n    pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv'),\n    pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv'),\n    on = 'id',\n    how = 'outer'\n).rename(columns = {'id': 'comment_id', 'comment_text': 'text'})\n\n# Combine all toxicity levels into one with the same weights set\ntmp_df['toxicity'] = sum([tmp_df[type] * coef for type, coef in toxicity_coefs.items()])\n\n# Add to the training data\ntrain_df = pd.concat([train_df, tmp_df.loc[tmp_df['toxicity'] >= 0]])\n\nprint(f\"Added {tmp_df.shape[0]} data to train_df. Now we have {train_df.shape[0]} entries.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:29:35.785510Z","iopub.execute_input":"2021-11-20T16:29:35.786170Z","iopub.status.idle":"2021-11-20T16:29:38.275805Z","shell.execute_reply.started":"2021-11-20T16:29:35.786112Z","shell.execute_reply":"2021-11-20T16:29:38.274559Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Smoothing Toxicity Scores\nIf your weights created too many *toxicity scores*, we can smooth them out.","metadata":{}},{"cell_type":"code","source":"# Convert our toxicity scores into a smoothed toxicity score\ntrain_df['smoothed_toxicity'] = train_df['toxicity'].apply(lambda x: 1 if x > 0 else 0)\n# train_df['smoothed_toxicity'] = train_df['toxicity']","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:29:38.277930Z","iopub.execute_input":"2021-11-20T16:29:38.278318Z","iopub.status.idle":"2021-11-20T16:29:38.425726Z","shell.execute_reply.started":"2021-11-20T16:29:38.278268Z","shell.execute_reply":"2021-11-20T16:29:38.424696Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Downsampling\nThe code below will work for any number of discrete *smoothed_toxicity* values and applies the cutoff accordingly.","metadata":{}},{"cell_type":"code","source":"# Be careful about the cutoff value. Too low value will likely make our model to underfit (We lose too many training data!)\nsmoothed_toxicity = train_df['smoothed_toxicity'].value_counts()\ncutoff = smoothed_toxicity.min()\n\n# Apply cutoff to each toxicity score and save them in a list\ncutoff_partitions = [train_df.loc[train_df['smoothed_toxicity'] == toxicity].sample(cutoff) for toxicity in smoothed_toxicity.index]\n\n# Concatenate and save them into the train_df\ntrain_df = pd.concat(cutoff_partitions)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:29:38.427432Z","iopub.execute_input":"2021-11-20T16:29:38.428167Z","iopub.status.idle":"2021-11-20T16:29:38.535978Z","shell.execute_reply.started":"2021-11-20T16:29:38.428127Z","shell.execute_reply":"2021-11-20T16:29:38.534870Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Text Cleaning\nAs language models improve, text-cleaning is becoming less necessary, but that's not the case for all models. My strategy is to start simple and test some cleaning methods to see if they help the model or not.","metadata":{"execution":{"iopub.status.busy":"2021-11-16T20:37:56.996117Z","iopub.execute_input":"2021-11-16T20:37:56.99644Z","iopub.status.idle":"2021-11-16T20:37:57.002838Z","shell.execute_reply.started":"2021-11-16T20:37:56.996406Z","shell.execute_reply":"2021-11-16T20:37:57.001704Z"}}},{"cell_type":"markdown","source":"### Validation: Defining our Validation Method\nWe need some sort of validation for our model to monitor its behavior. One way to accomplish this is to use the data from *validation_data.csv*. We can use our model to predict on both *less_toxic* and *more_toxic* columns and comparing the results.","metadata":{}},{"cell_type":"code","source":"# Defining the validation function\ndef validate_pipe(pipe, less_toxic, more_toxic):\n    \n    prob_1 = pipe.predict_proba(less_toxic)\n    prob_2 = pipe.predict_proba(more_toxic)\n    \n    return (prob_1[:, 1] < prob_2[:, 1]).mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:29:38.540600Z","iopub.execute_input":"2021-11-20T16:29:38.540896Z","iopub.status.idle":"2021-11-20T16:29:38.546978Z","shell.execute_reply.started":"2021-11-20T16:29:38.540863Z","shell.execute_reply":"2021-11-20T16:29:38.546180Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Pipeline","metadata":{}},{"cell_type":"code","source":"# Define pipeline\npipe = Pipeline([\n    ('vectorizer', TfidfVectorizer(stop_words = 'english')),\n    ('model', MultinomialNB())\n])\n\n# Train the pipeline\npipe.fit(train_df['text'], train_df['smoothed_toxicity'])\n\n# Validate\nvalidate_pipe(pipe, val_df['less_toxic'], val_df['more_toxic'])","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:30:04.460710Z","iopub.execute_input":"2021-11-20T16:30:04.461178Z","iopub.status.idle":"2021-11-20T16:30:12.702455Z","shell.execute_reply.started":"2021-11-20T16:30:04.461145Z","shell.execute_reply":"2021-11-20T16:30:12.701356Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Creating Submission","metadata":{}},{"cell_type":"code","source":"y_pred = pipe.predict_proba(test_df['text'])\n\nsubmission_df = pd.DataFrame(data = {\n    'comment_id': test_df['comment_id'],\n    'score': y_pred[:, 1]\n}).to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:30:16.461404Z","iopub.execute_input":"2021-11-20T16:30:16.462059Z","iopub.status.idle":"2021-11-20T16:30:17.081730Z","shell.execute_reply.started":"2021-11-20T16:30:16.462023Z","shell.execute_reply":"2021-11-20T16:30:17.080740Z"},"trusted":true},"execution_count":13,"outputs":[]}]}